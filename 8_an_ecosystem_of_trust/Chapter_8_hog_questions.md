# Human Oversight Guidelines from Subject Matter Experts (SME)

<a id="top"></a>

## Contents
[Privacy SME](#privacy-sme)

[Use Case SME](#use-case-sme)

## Privacy SME

### P1. What types of data fall under the purview of a privacy review?

Here are some broad, non-exhaustive categories:

- *Personal Identifiable Information (PII)*: This includes any data that can be used to directly or indirectly identify an individual, such as names, addresses, email addresses, phone numbers, social security numbers, or any other unique identifier.

- *Sensitive Personal Information*: This refers to data that, if disclosed, could lead to significant harm or discrimination, such as financial information, health information, biometric data, religious or philosophical beliefs, racial or ethnic origin, political opinions, or sexual orientation.

- *User-generated Content*: Any content or information generated by users, such as comments, posts, photos, videos, or other forms of user-generated content, may require privacy considerations to ensure proper handling and protection.

- *Location Data*: Any data that indicates the geographical location of an individual, including GPS coordinates, IP addresses, or information obtained from mobile devices, falls under the scope of a privacy review.

- *Behavioral Data*: This refers to data collected about an individual's online activities, such as browsing history, search queries, clickstream data, or interactions with digital platforms. It is important to review the privacy implications of collecting and analyzing such data.

- *Metadata*: Metadata includes information about the data itself, such as timestamps, data source, user IDs, or transaction logs. Although metadata may not directly identify individuals, it can still reveal patterns or insights that require privacy considerations.

### P2. What laws/regulations are there for this data and its use?

Based on the geographic region the data elements from an individual are collected
from, one or more laws may apply. GDPR applies to data—including what is publicly
available—obtained from a natural person from the EU. In the US, data on residents of California, Virginia, and Colorado is subject to protection under laws in the respective jurisdictions. De-identified and publicly available data does not fall under these laws, but aggregate information does in Virginia and Colorado. You can consult [Bloomberg Law](https://pro.bloomberglaw.com/brief/privacy-laws-us-vs-eu-gdpr/) for more information or reach out to me.

### P3. What type of privacy risks should ML projects be aware of when using this data?

- *Data Breaches* can result in unauthorized access or disclosure of sensitive data. Implementing robust security measures, such as encryption, access controls, and monitoring systems, is essential to mitigate this risk.

- *Re-identification*: It is crucial to assess and implement appropriate anonymization techniques to minimize the risk of re-identification, such as data aggregation, noise addition, or differential privacy methods.

- *Bias and Discrimination*: ML models trained on privacy-sensitive data can perpetuate biases or discrimination if the data itself contains such biases. 

- *Unauthorized Secondary Uses*: Privacy-sensitive data should be used only for the intended purposes. Clear guidelines and controls should be established to prevent unauthorized secondary uses of the data, minimizing the risk of using the data beyond the scope of consent or legal obligations.

- *Informed Consent*: ML projects must consider the privacy implications when obtaining consent from individuals whose data is being used. Ensuring informed consent includes providing clear information on the specific data collected, its intended uses, and any potential risks or consequences.

### P4. Are there external examples of these risks? If so, please supply those examples.

- *Data Breaches*: In 2019, Capital One [experienced](https://www.capitalone.com/digital/facts2019/) a data breach---due to a misconfigured web application firewall---where the personal information of over 100 million customers was compromised.

- *Re-identification*: In 2006, AOL [released](https://en.wikipedia.org/wiki/AOL_search_log_release) a dataset containing search queries of over 650,000 users. Although the data was anonymized, researchers were able to re-identify individuals based on their search queries.

- *Unauthorized Secondary Uses*: Facebook [faced controversy](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal) in 2018 when it was revealed that the personal data of millions of users was improperly shared with third-party companies, such as Cambridge Analytica. The data was collected for one purpose but was subsequently used for targeted political advertising without proper consent.

- *Informed Consent*: In 2018, the mobile app Strava [unintentionally revealed](https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases) potentially sensitive location data of military personnel. The app publicly displayed heatmaps of user activities, inadvertently disclosing the locations and routines of military bases and personnel without explicit informed consent.

### P5. What characteristics would enable an ML team to assess whether a project is low, medium, or high risk? Are there ways to mitigate related risks?

Assessing the risk level of an ML project involves considering various characteristics and factors. While the specific criteria may vary depending on the context and nature of the project, the following characteristics can help determine the risk level:

- *Data Sensitivity*: Evaluate the sensitivity of the data involved in the project. Privacy-sensitive data, such as personally identifiable information (PII), health information, or financial data, increases the risk level compared to non-sensitive data.

- *Data Volume and Granularity*: Consider the volume and granularity of the data. Larger datasets with more detailed information may pose higher risks due to increased potential for re-identification or unintended disclosures.

- *Regulatory and Legal Requirements*: Assess the extent to which the project complies with relevant privacy laws and regulations. Projects that involve personal data and fall under strict regulatory frameworks, such as GDPR or HIPAA, are generally associated with higher risks.

- *Impact on Individuals*: Evaluate the potential impact on individuals if their privacy is compromised. Projects that could lead to significant harm, discrimination, or reputational damage pose higher risks.

- *Algorithm Complexity*: Consider the complexity of the ML algorithms used. Complex models, particularly those with black-box decision-making processes, may pose challenges in assessing potential biases, transparency, and potential privacy breaches.

- *Data Sharing and Third-Party Involvement*: Assess whether the project involves sharing data with external parties or third-party dependencies. Increased data sharing or involvement of external entities raises the risk of unauthorized data usage or breaches.

### P6. Are there data elements that need specific approval to use in risk mitigation?

Yes, specific data elements require approvals or considerations for risk mitigation purposes.

- *Sensitive Personal Data*: The use of SPI requires explicit consent or additional safeguards to mitigate the associated risks. Please reach out to the Privacy team for the approval process if you plan to use SPI in your ML work.

- *Children's Data*: If a ML model being built is trained on children's data or its inferences served to children, the privacy team should be involved to ensure that parental consent flags are used in model development and deployment pipelines.

- *Special Categories of Data*: Under regulations like the General Data Protection Regulation (GDPR), special categories of data, such as racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, or data concerning health, require privacy review to justify their use.

- *Data Subject Consent*: Depending on the jurisdiction and the applicable data protection laws, certain data elements or processing activities may require obtaining explicit consent from the data subjects. Consent should be freely given, informed, and specific to the purposes for which the data is being used.

- *Data Transfer Restrictions*: If the data is being transferred across borders, specific approvals or mechanisms, such as standard contractual clauses or binding corporate rules, may be necessary to ensure the data is adequately protected during the transfer.

### P7. What metrics are typically used to measure privacy risks? Are there standard acceptable thresholds for these metrics?

Measuring privacy risks involves the assessment of various metrics to evaluate the potential impact on individuals' privacy. While there are no universally standardized metrics or fixed acceptable thresholds, here are some commonly used metrics for measuring privacy risks:

- *Identifiability*: The metric quantifies the risk of re-identification of individuals from the available data. It may consider factors such as the uniqueness of data combinations, data granularity, and the presence of direct or indirect identifiers.

- *Information Sensitivity*: This metric evaluates the sensitivity of the data being processed, considering factors such as personally identifiable information (PII), health information, financial data, or other sensitive categories of data.

- *Consent Compliance*: The metric assesses whether the data processing practices comply with the consent obtained from the individuals. It involves evaluating whether the collected data is used within the boundaries specified in the consent agreement.

- *Data Minimization*: This metric examines the extent to which data collection, retention, and processing practices adhere to the principle of data minimization. It measures the level of necessity and proportionality in handling personal data.

- *Security Measures*: This metric assesses the effectiveness of security measures implemented to protect data from unauthorized access, breaches, or accidental disclosures. It considers factors such as encryption, access controls, security audits, and incident response protocols.

- *Compliance with Regulations*: This metric evaluates the project's adherence to relevant privacy regulations and legal requirements. It includes assessing whether the project meets the obligations set forth in laws such as GDPR, CCPA, or sector-specific regulations.

While there are no standard acceptable thresholds for these metrics, we have internal guidelines based on regulatory requirements, industry best practices, and their risk appetite. Organizations may also refer to privacy frameworks or guidelines, such as Privacy Impact Assessment (PIA) methodologies, to help assess and manage privacy risks effectively.

### P8. What vetting is done for third-party data, and what liabilities do we risk in using the data?

Our vetting process for third-party data typically involves the following steps:

- *Due Diligence*: We conduct thorough due diligence on the data provider, assessing their reputation, credibility, and track record. This includes evaluating the provider's data collection practices, data sources, data quality assurance procedures, and compliance with relevant regulations.

- *Data Quality Assessment*: We assess the quality and accuracy of the third-party data. This may involve evaluating data collection methods, data validation processes, data cleansing techniques, and data maintenance practices.

- *Legal and Regulatory Compliance*: We verify that the third-party data is obtained and processed in compliance with applicable laws and regulations. This includes ensuring that the data provider has obtained necessary consents, has the right to share the data, and has appropriate data transfer mechanisms in place if cross-border transfers are involved.

- *Data Security and Privacy*: We assess the data provider's data security measures, including encryption, access controls, and safeguards against unauthorized access or data breaches. Additionally, the company should evaluate the data provider's privacy practices, ensuring they align with privacy principles and protect individuals' rights.

Using third-party data carries certain liabilities and risks for the company. These can include:

- *Legal and Regulatory Compliance*: If the third-party data is obtained or used in violation of applicable laws or regulations, the company may face legal consequences, fines, or reputational damage. It is important to ensure that the data provider follows proper legal and ethical standards.

- *Data Accuracy and Reliability*: Relying on inaccurate or unreliable third-party data can lead to flawed insights, incorrect decisions, and potential harm to individuals or business operations. The company may be held liable for any damages resulting from the use of unreliable data.

- *Data Breaches and Security Incidents*: If the third-party data is compromised due to security vulnerabilities or breaches on the data provider's side, the company may face reputational damage, legal liabilities, and regulatory penalties for failing to protect the privacy and security of the data.

- *Privacy Violations*: If the third-party data includes personal information, its use must comply with privacy regulations. Inappropriate or unauthorized use of personal data can lead to privacy violations, complaints, legal actions, and reputational harm.

To mitigate these risks, the company should establish clear contracts and agreements with the data provider, outlining data usage, data protection, security requirements, and compliance obligations. Regular monitoring and audits of the data provider's practices may also be necessary to ensure ongoing compliance and mitigate potential liabilities.

### P9. Are there privacy concerns on reuse of data/model elements?

Yes. These concerns primarily revolve around the potential for unintended or unauthorized uses of the data or model elements, which can compromise privacy. Here are some specific privacy concerns related to data and model element reuse:

- *Data Combination and Linkage*: Reusing data or model elements from different sources or projects may increase the risk of combining or linking information that can lead to the identification of individuals. The aggregation of previously separate datasets can create new insights that breach privacy expectations.

- *Data Leakage*: Reusing data or model elements without proper safeguards increases the risk of data leakage. If data elements contain personally identifiable information (PII) or other sensitive data, their reuse without adequate protection measures can result in unauthorized access or disclosure, potentially leading to privacy breaches.

- *Model Transferability*: Reusing trained ML models or model elements across different applications or contexts can inadvertently transfer biases or discriminatory patterns. If the models were trained on data that contains biases or sensitive attributes, their reuse may perpetuate those biases and result in discriminatory outcomes.

### P.10 Whom should a data scientist contact for additional information?

A data scientist seeking additional information within a company can contact various stakeholders. These include the Data Protection Officer (DPO) responsible for privacy compliance, the privacy or legal department for legal guidance, the information security team for data security matters, and the data governance team for data management practices. Subject matter experts can provide domain-specific insights, while internal compliance or risk management teams can assist with regulatory compliance and risk assessments. Project managers or team leads can offer project-specific guidance and resources. Engaging with these stakeholders ensures compliance, addresses privacy concerns, and accesses the necessary information for data science projects. Collaboration and communication with relevant teams and experts enable data scientists to make informed decisions and execute projects effectively while maintaining privacy and compliance.

[back to top](#top)

## Use case SME

### U1. Usage of what types of data can have trust concerns in the domain of targeted advertising?

Usage of sensitive personal information (SPI) and personally identifiable information (PII) is important to think about from a privacy angle. Using PII is not allowed, while using SPI such as racial origin or geolocation needs approval. If there are features in the data that correlate with sensitive features such as race and gender, there may be fairness issues if you do not account for the sensitive features in the ML pipeline.

### U2. What privacy, legal, or other requirements are there to get access to such data?

Accessing data for targeted advertising involves privacy, legal, and other requirements. Key considerations include obtaining explicit user consent, transparent privacy policies, compliance with data protection regulations (e.g., GDPR, CCPA), data security measures, opt-out mechanisms, adherence to advertising standards and ethics, and addressing cross-border data transfers. Advertisers must inform users about data collection, usage, and any third parties involved. Privacy policies should be clear, accessible, and written in plain language. Compliance with data protection laws is crucial, including data minimization, security measures, and privacy impact assessments. Advertisers should provide options for users to control their data, respect opt-outs promptly, and implement measures to prevent unauthorized access and data breaches. Adherence to advertising standards and ethical guidelines builds trust. Considerations for cross-border data transfers may involve using contractual clauses or ensuring adequate data protection in recipient countries. Staying updated with evolving privacy laws, regulations, and industry standards is essential for responsible targeted advertising.

### U3. Is it possible to use publicly available or aggregated data to avoid trust issues?

Using publicly available or aggregated data can partially address trust issues in targeted advertising. Publicly available data, such as demographic information or general market research, can offer insights without breaching privacy. Aggregated data combines multiple sources, minimizing the risk of individual identification. However, trust concerns may still arise. Aggregated data can indirectly reveal personal information, and users may worry about being tracked and targeted. Transparent communication is essential to address these concerns. Advertisers should clearly disclose data sources, targeting purposes, and provide opt-out options. Respecting user choices and ensuring data security are vital to foster trust. Regardless of the data type used, responsible data handling, privacy protection, and user consent remain critical in targeted advertising.

### U4. What laws/regulations are there in this domain?

In the domain of targeted advertising, several laws and regulations govern the collection, use, and protection of user data. The specific laws vary across jurisdictions, but here are some notable examples:

- *General Data Protection Regulation (GDPR)*: Enforced in the European Union, the GDPR sets guidelines for the lawful processing of personal data, emphasizing user consent, data minimization, purpose limitation, and individual rights such as the right to access and erasure.

- *California Consumer Privacy Act (CCPA)*: Applicable to businesses operating in California, the CCPA grants consumers rights over their personal information, including the right to know what data is collected, the right to delete data, and the right to opt-out of data sales.

- *ePrivacy Directive*: Implemented in the EU member states, the ePrivacy Directive focuses on privacy rights related to electronic communications. It covers rules for the use of cookies, direct marketing communications, and confidentiality of communications.

- *Children's Online Privacy Protection Act (COPPA)*: In the United States, COPPA protects children's privacy by requiring verifiable parental consent for the collection and use of personal information of children under the age of 13.

- *Digital Advertising Alliance (DAA) Self-Regulatory Principles*: The DAA, an industry self-regulatory body, establishes guidelines for responsible digital advertising practices, including transparency, consumer control, and choice mechanisms.

- *Advertising Standards and Industry Codes of Conduct*: Many countries have advertising standards bodies or industry associations that set guidelines and codes of conduct specific to advertising practices, including targeted advertising. These organizations promote ethical and responsible advertising practices.

It is crucial for businesses engaged in targeted advertising to stay informed about the legal and regulatory landscape in their operating regions. Compliance with these laws and regulations helps protect user privacy, ensure transparency, and foster trust between advertisers and consumers.

### U5. What metrics, thresholds, and mitigation techniques can be used to detect and mitigate trust issues in advertising use cases?

To detect and mitigate trust issues in advertising use cases, several metrics, thresholds, and mitigation techniques can be employed:

- *Data Transparency Metrics*: Track and report on metrics related to data collection, usage, and sharing practices. This includes providing clear disclosures on the types of data collected, the purposes for which it is used, and any third parties involved.

- *Consent Metrics*: Monitor consent rates and ensure that users are provided with meaningful choices and options for managing their data preferences. Assess the clarity and accessibility of consent mechanisms to gauge user understanding and engagement.

- *User Complaints and Feedback*: Establish channels for users to provide feedback and report concerns. Monitor and analyze user complaints and feedback to identify patterns, common issues, and areas of improvement.

- *Ad Performance Metrics*: Assess ad performance indicators such as click-through rates, conversion rates, and engagement metrics. Identify any significant discrepancies or anomalies that could indicate unethical or misleading advertising practices.

- *Ad Content Review*: Implement a robust ad content review process to ensure compliance with advertising standards, ethical guidelines, and legal requirements. Regularly monitor ads for false claims, deceptive practices, or offensive content.

- *Auditing and Compliance Checks*: Conduct regular audits and compliance checks to ensure adherence to privacy regulations, data protection requirements, and industry standards. This includes reviewing data handling practices, data security measures, and data sharing agreements.

- *Privacy Impact Assessments*: Perform privacy impact assessments to identify potential privacy risks and mitigation strategies. Assess the impact of targeted advertising activities on user privacy and implement measures to minimize risks.

- *User Education and Transparency*: Promote user education and transparency by providing clear information on how targeted advertising works, the benefits it offers, and how user privacy is protected. Foster trust by being transparent about data practices and offering user-friendly privacy controls.

By monitoring these metrics, establishing appropriate thresholds, and implementing mitigation techniques, advertisers can detect and address trust issues proactively. This helps maintain ethical advertising practices, protect user privacy, and build trust between advertisers and consumers.

### U6. Who are the resource persons outside the ML team who may be consulted?

Outside the ML team, several resource persons can be consulted in the domain of targeted advertising to gain insights and expertise. Some key individuals or teams include:

- *Legal and Compliance Experts*: Lawyers specializing in privacy laws, data protection regulations, and advertising compliance can provide guidance on legal requirements, assist in drafting privacy policies, and ensure adherence to relevant regulations.

- *Privacy Officers or Data Protection Officers (DPOs)* play a crucial role in overseeing data protection and privacy practices within organizations. Consulting with DPOs can help ensure compliance with privacy regulations and address trust concerns effectively.

- *Ethical Review Boards* can provide valuable perspectives on ethical considerations and potential social implications, In cases where sensitive data or potentially controversial targeting is involved.

- *Industry Associations and Standards Bodies* such as the Interactive Advertising Bureau (IAB), Network Advertising Initiative (NAI), or Digital Advertising Alliance (DAA) offer guidelines, best practices, and industry standards in targeted advertising.

- *User Advocacy Groups and Privacy Organizations* can provide valuable insights on privacy issues and help shape responsible advertising strategies. Engaging with them allows for understanding user perspectives, concerns, and expectations.

- *Market Research Firms* can offer insights into consumer behavior, preferences, and trends, helping shape effective and targeted advertising strategies while considering trust concerns.

Consulting these resource persons outside the ML team can provide diverse expertise and perspectives, ensuring a comprehensive approach to targeted advertising that considers legal, ethical, privacy, and user-centric aspects.